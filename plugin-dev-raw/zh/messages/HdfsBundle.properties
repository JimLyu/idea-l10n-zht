group.names.hdfs.data=Hdfs issues

# Connection settings
client.is.not.inited=Client is not initialized

s3.operations.timeout.emptyText=Optional custom timeout in seconds for operations
s3.operations.timeout.hint=Timeout for operations on the remote server in seconds

settings.s3.selection.endpoint=與 S3 相容的存儲
settings.s3.selection.region=Selected region
settings.s3.custom.endpoint=端點:
settings.s3.custom.region=區域:
settings.s3.custom.region.hint=Use if required
settings.s3.region=區域:
settings.config.path=Configs path:
settings.custom.roots=根:
settings.bucket.filter=Buckets filter:
settings.bucket.filter.type=Filter type:
settings.s3.access.key=Access key:
settings.s3.secret.key=Secret key:
settings.s3.profile.use.custom.path=Use custom configs
settings.s3.bucket.filter.by.region=Only buckets in the selected region
settings.s3.profile.name=設定檔名:
settings.s3.profile.credentials.path=Credentials path:
settings.s3.profile.config.path=Config path:
settings.s3.auth.type=身份驗證:
settings.azure.auth.type=身份驗證:
settings.azure.endpoint=端點:
settings.azure.username=用戶名:
settings.azure.password=密碼:
settings.azure.sas.token=SAS Token:
settings.azure.connection.string=Connection string:
settings.azure.user.key=密鑰:
settings.azure.container=容器:
settings.hdfs.auth.type=身份驗證:
settings.hdfs.url=File system URI:
settings.hdfs.username=用戶名:
settings.minio.endpoint=端點:
setup.video.tutor=Connection setup tutorial video
settings.undefined.path=<未初始化>

# Connection group names
group.name.hdfs.java=HDFS
group.name.gcs=Google Cloud Storage
group.name.azure=Azure


# GCS settings
gcs.project.id=專案 ID:
gcs.project.id.emptyText=Optional overwrite Project ID
gcs.project.id.hint=Show buckets for special Project ID
gcs.json.location=Google app credentials:
gcs.json.location.emptyText=cloud store JSON location
gcs.buckets.source=Buckets source:
gcs.custom.url=Custom host:
gcs.public.hint=Leave it blank for public buckets
gcs.connection.browse.title=Select Credentials JSON
gcs.connection.error.file.not.exists=檔案不存在
gcs.connection.error.cred.file.not.selected=Credentials file must be selected for the account
gcs.connection.error.bucket.validation1=Bucket names must contain 3-63 characters. Names containing dots can contain up to 222 characters, but each dot-separated component can be no longer than 63 characters.
gcs.connection.error.bucket.validation2=Names must contain only lowercase letters, numbers, dashes (-), underscores (_), and dots (.).
gcs.connection.error.bucket.validation3=Bucket names must start and end with a number or letter.
gcs.connection.error.bucket.validation4=Bucket names cannot be represented as an IP address in dotted-decimal notation (for example, 192.168.5.4).
gcs.connection.error.bucket.validation5=Bucket names cannot begin with the "goog" prefix.
gcs.connection.error.bucket.validation6=Bucket names cannot contain "google" or close misspellings, such as "g00gle".

# HDFS java settings
hdfs.java.driver.home.path=Driver home path:
hdfs.java.config.source=Configuration source:
hdfs.config.path.title=Java API Configs Path
hdfs.config.path.not.empty=Configuration path should not be empty
hdfs.config.path.does.not.exist=Specified directory does not exist
hdfs.config.path.should.be.directory=Configuration path should point to a directory
hdfs.config.path.no.xmls.found=Specified directory does not contain any XML files

# S3 settings
s3.auth.type=Authentication type:
s3.proxy.preemptiveBasicProxyAuth=Preemptive basic proxy auth
s3.proxy.preemptiveBasicProxyAuth.hint=Whether to pre-emptively authenticate against a proxy server using basic authentication
s3.proxy.disableSocketProxy=Disable socket proxy
s3.proxy.disableSocketProxy.hint=Set whether to disable proxies at the socket level.
s3.proxy.nonProxyHosts=Non proxy hosts:
s3.proxy.nonProxyHosts.hint=Optional specifies the hosts that should be accessed without going through the proxy.
s3.proxy.ntlm=NTLM Proxy Support
s3.proxy.workstation=Proxy workstation:
s3.proxy.workstation.hint=Optional Windows workstation name for configuring NTLM proxy support.
s3.proxy.domain=Proxy domain:
s3.proxy.domain.hint=Optional Windows domain name for configuring NTLM proxy support.
s3.empty.directories.not.allowed=Creating empty directories is not allowed

# SFTP settings
group.name.s3=AWS S3
group.name.dospaces=DigitalOcean 空間
group.name.linode=Linode
group.name.minio=MinIO

#HDFS error messages
copy.failed=Copy from {0} to {1} has failed.
move.failed=Move from {0} to {1} has failed.
hdfs.ssh.tunnel.ssh.operation.not.supported=Operation is not supported through SSH Tunnel.
ssh.additional.label=\ (Only NameNode operations)
ssh.additional.info=SSH Tunneling works <b>only for operations with the name node</b>: list files, get meta info.<br><br>
azure.rename.text2.indicator.deleting={0}: Deleting {1}
connection.error.root.path.must.be.non.empty=Root path must be non-empty
hdfs.field.root.path=根路徑
hdfs.no.xmls.in.directory=No xml files in configuration root
hdfs.is.not.inited=Hdfs connection is not inited
hdfs.root.folder.does.not.exist=Root folder {0} does not exist
notification.title.error.while.reading.orc=Error while reading ORC
notification.content.orc.file.looks.invalid.we.can.t.open.it.if.you.believe.that.s.wrong.please.create.issue.at.https.youtrack.jetbrains.com.issues.bdide=ORC file looks invalid and cannot be opened. If you know that the file is valid, create an issue at https://youtrack.jetbrains.com/issues/BDIDE
avro.file.is.broken.length.zero=File {0} is broken. Length: 0
gcs.progress.details.deleting=正在刪除 {0}
cannot.open.read.stream.null.blob=Cannot open read stream for the null blob {0}
s3.connection.error.ssh.without.endpoint=To use SSH Tunnel, specify an endpoint for the driver
error.create.ssh.tunnel=Cannot create SSH Tunnel
group.name.emr=AWS EMR
emr.is.not.inited=EMR client is not initialized
emr.toolwindow.title=AWS EMR

emr.cluster.terminate.cluster.message=Do you want to terminate the cluster {0}?
emr.cluster.terminate.cluster.title=Cluster Terminating
controller.cluster.steps.error=Cluster Steps update error
emr.cluster.filter=Filter By State
emr.cluster.filter.limit=Limit
emr.filter.text=篩選器:
cannot.find.linode.region=Cannot find the region for {0}
rfs.create.bucket.message=建立存儲桶
emr.spark.submit.editor.name=名稱:
emr.spark.submit.editor.args=實參:
emr.spark.submit.editor.jar.loc=JAR location:
emr.spark.submit=EMR Spark-submit
s3.filter.type.match=Match
s3.filter.type.contains=Contain
s3.filter.type.regex=正則表達式
s3.filter.type.startwith=Start with

s3.multibucket.update.text=Multi-buckets is supported for S3-compatible storages. You can configure them in the connection settings.
s3.multibucket.update.title=BigDataTools new S3 feature
s3.multibucket.open.settings=開啟設定
s3.bucket.text.empty=All buckets are visible
s3.bucket.text.hint=If this field is empty, all buckets will be visible<br>Type a bucket name and choose filter type "match" to work with a single bucket<br>Use "," to separate buckets (bucket1,bucket2)
emr.error=AWS EMR Exception

# Auth type
auth.type.default=Default credential providers chain
auth.type.keypair=Explicit access key and secret key
auth.type.namedprofile=Named profile
auth.type.credentialsfile=Profile from credentials file
connection.error.hadoop.home.is.not.defined.full=HADOOP_HOME is not defined. On Windows you should have the HADOOP_HOME environment variable defined or Java property hadoop.home.dir. Please, refer to <a href=\"https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\">Hadoop Wiki</a> for more details.
connection.error.hadoop.no.native.drivers.full=Unable to find native drivers in HADOOP_HOME. Please, refer to <a href=\"https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\">Hadoop Wiki</a> for more details.
connection.error.hadoop.password.is.not.found=The password was not found
connection.error.hadoop.credentials.is.not.found=The credential was not found

emr.step.mapper.choose=Select Mapper
emr.step.reducer.choose=Select Reducer
emr.step.s3.input.choose=Select S3 Input
emr.step.s3.output.choose=Select S3 Output

emr.step.script.choose=Select script S3 location

emr.label.choose.key.file.for.aws.pair=Choose key file for AWS {0} pair
emr.dialog.title.select.key.ssh.file=Select The Key SSH File
controller.cluster.instances.error=Update cluster instances error

aws.cluster.info.action.open.subnet=Open Subnet
aws.cluster.info.action.open.master.security.group=Master Security Group
aws.cluster.info.action.open.slave.security.group=Core and Tasks Security Group

emr.step.details=Show Step Details
emr.cluster.info.details=顯示為 JSON

emr.remove.linked.connections.title=Emr connections
emr.remove.linked.connections.desc=Do you want to delete connections which were created for EMR?
emr.remove.linked.connections.action=Remove Connections
emr.keys.settings.table.empty=No SSH Keys provided
emr.keys.settings.column.key.name=Key Name
emr.keys.settings.column.key.path=路徑
emr.keys.settings.label=SSH Keys:
emr.keys.settings.link=Open SSH Key Settings
emr.key.storage.dialog.title=EMR SSH Keystore

group.name.yandex=Yandex Object Storage

# DigitalOceanRegions
do.region.nyc3=New York City, United States
do.region.ams3=Amsterdam, the Netherlands
do.region.sfo=San Francisco, United States
do.region.sgp1=Singapore
do.region.fra1=Frankfurt, Germany

# LinodeRegions
linode.region.us-east=US-East (Newark, USA)
linode.region.us-southeast=US-Southeast (Atlanta, USA)
linode.region.eu-central=EU-Central (Frankfurt, Germany)
linode.region.ap-south=AP-South (Singapore)

group.name.alibaba=Alibaba OSS

settings.alibaba.region=區域:
alibaba.bucket.create.dialog.hierarchical.field=分層命名空間
alibaba.task.delete.directory.text2=Already deleted {0} objects
alibaba.task.delete.directory.text=Deleting directory {0}
alibaba.task.delete.file.text=Deleting file {0}
alibaba.task.delete.bucket.text=Deleting bucket {0}
cannot.find.buckets.by.filter=Cannot find any buckets by filter "{0}" with text "{1}"

# AlibabaRegions
alibaba.region.oss-cn-hangzhou=China (Hangzhou)
alibaba.region.oss-cn-shanghai=China (Shanghai)
alibaba.region.oss-cn-qingdao=China (Qingdao)
alibaba.region.oss-cn-beijing=China (Beijing)
alibaba.region.oss-cn-zhangjiakou=China (Zhangjiakou)
alibaba.region.oss-cn-huhehaote=China (Hohhot)
alibaba.region.oss-cn-wulanchabu=China (Ulanqab)
alibaba.region.oss-cn-shenzhen=China (Shenzhen)
alibaba.region.oss-cn-heyuan=China (Heyuan)
alibaba.region.oss-cn-guangzhou=China (Guangzhou)
alibaba.region.oss-cn-chengdu=China (Chengdu)
alibaba.region.oss-cn-hongkong=China (Hong Kong)
alibaba.region.oss-us-west-1=US (Silicon Valley)
alibaba.region.oss-us-east-1=US (Virginia)
alibaba.region.oss-ap-southeast-1=Australia (Sydney) 1
alibaba.region.oss-ap-southeast-2=Australia (Sydney) 2
alibaba.region.oss-ap-southeast-3=Malaysia (Kuala Lumpur)
alibaba.region.oss-ap-southeast-5=Indonesia (Jakarta)
alibaba.region.oss-ap-northeast-1=Japan (Tokyo)
alibaba.region.oss-ap-south-1=India (Mumbai)
alibaba.region.oss-eu-central-1=Germany (Frankfurt)
alibaba.region.oss-eu-west-1=UK (London)
alibaba.region.oss-me-east-1=UAE (Dubai)
alibaba.region.oss-ap-southeast-6=Philippines (Manila)
wrong.region=Cannot find region {0}

alibaba.settings.credentials.file=Alibaba Credentials File
dialog.title.s3.profiles.credentials.file=憑證檔案
dialog.title.s3.profiles.config.file=Config File
open.credentials=Open Credentials
s3.column.name.storage.class=存儲類別
s3.column.name.metadata=元資料
s3.column.name.etag=ETag
file.info.label.key=密鑰:
file.info.label.region=區域:
file.info.label.owner=所有者:
file.info.label.creation.date=Creation date:
file.info.label.storage.class=存儲類別:
file.info.label.etag=ETag:
s3.file.info.label.uri=S3 URI:
file.info.label.object.url=物件 URL:
oss.file.info.label.type=類型:
oss.file.info.label.hns.status=分層命名空間:
oss.file.info.label.region=區域:
oss.file.info.label.storage.class=存儲類別:
oss.file.info.label.resource.group=Resource group:
oss.file.info.label.location=位置:
oss.file.info.label.extranet.endpoint=External endpoint:
oss.file.info.label.intranet.endpoint=Internal endpoint:
file.info.label.created=建立時間:
file.info.label.modified=Last modified:
file.info.label.media.link=Media link:
azure.column.name.access.tier=Access Tier
azure.column.name.access.modified=Access Tier Last Modified
azure.column.name.blob.type=Blob Type
file.info.access.tier=Access tier
file.info.access.tier.modified=Access tier last modified
file.info.access.blob.type=Blob type
file.info.access.content.type=Content type
error.object.summary.is.not.found=No object summary for {0}
settings.buckets.user.list=All buckets in the account
settings.buckets.custom.list=Custom roots
settings.buckets.hint=<html><b>All buckets in the account</b> - perform sort of <it>list buckets</it> request. Allow filter the result bucket list.<br><br><b>Custom roots</b> - request selected roots directly, allow specifying not just buckets, but full path to directory.</html>
custom.bucket.text.empty=bucket/folder,bucket2/folder/subfolder2,...
custom.bucket.text.hint=Use "," separator to specify list of source roots (bucket1/folder1/folder2,bucket2/folder)
gcs.multibucket.update.title=BigDataTools new GCS feature
gcs.multibucket.update.text=Multi-buckets are supported for Google Cloud Storages! You can configure them in the connection settings.
hdfs.column.name.group=組
hdfs.column.name.owner=所有者
hdfs.column.name.access.time=Accessed
hdfs.column.name.block.size=Block Size
hdfs.column.name.permission=Permission
hdfs.column.name.is.encrypted=Encrypted
hdfs.column.name.is.isSnapshotEnabled=快照
hdfs.column.name.is.isErasureCoded=Erasure Coded
hdfs.column.name.replications=副本
hdfs.file.info.label.group=組:
hdfs.file.info.label.owner=所有者:
hdfs.file.info.label.accessTime=Access time:
hdfs.file.info.label.modificationTime=Modification time:
hdfs.file.info.label.block.size=Block size:
hdfs.file.info.label.replication=副本:
hdfs.file.info.label.permission=Permission:
hdfs.file.info.label.isEncrypted=Encrypted:
hdfs.file.info.label.isSnapshotEnabled=Snapshot enabled:
hdfs.file.info.label.isErasureCoded=Erasure coded:
hdfs.file.info.label.size=大小:
emr.error.stop.cluster=Stop Cluster Error
emr.connection.warning.no.clusters=Connected, clusters aren't found
emr.connection.warning.no.clusters.desc=Connection is established, but clusters are not found for the selected region. Please, check that the region is correct.
emr.connection.warning.no.clusters.desc.window=Clusters are not found in {0} region. Please, check the region.
emr.connection.creation=EMR Create Connection

bucket.name.is.empty.for.path=Bucket name is empty for path "{0}"
auth.type.anon=匿名
connection.error.profile.config.file.is.not.found=Config file is not found by path {0}
connection.error.profile.creds.file.is.not.found=Credentials file is not found by path {0}

java.wrong.path.inspection.description=Highlight invalid hdfs paths in java code
kotlin.wrong.path.inspection.description=Highlight invalid hdfs paths in kotlin code
scala.wrong.path.inspection.description=Highlight invalid hdfs paths in scala code
invalid.format.inspection.description=Highlight custom hdfs formats. By defaults formats parquet, orc, sequence, json, csv or text are expected.
invalid.format.inspection.template=Unexpected custom file format

# Inspections and highlightings
scala.serializable.scope.inspection.description=Highlight non-serializable values used in spark task scopes that would result in runtime exception.
scala.serializable.scope.inspection.warning=<html>Non-serializable value {0} of type {1} in spark scope {2}</html>

#Metainfo
gcs.metainfo.dialog.title=Blob Metadata
gcs.metainfo.dialog.column.key=鍵
gcs.metainfo.dialog.column.value=值
gcs.metainfo.section.title=元資料
gcs.metainfo.section.link.edit=Edit metadata

metainfo.section.custom.headers=標頭
metainfo.section.custom.headers.link.edit=Edit custom headers...
emr.dialog.title.select.key.info.title=SSH Key Required
emr.dialog.title.select.key.info.cancel=取消
emr.dialog.title.select.key.info.ok=Select SSH Key
emr.dialog.title.select.key.info.msg=The connection requires creating an SSH tunnel. To set it up, please select the SSH key file of your cluster.
settings.ssl.trust.all=Trust all SSL certificates
file.info.label.bucket=存儲桶:
metainfo.section.tags=標記
metainfo.section.tags.link.edit=編輯標記…
metainfo.section.custom.headers.loading=正在載入檔案元資訊…
metainfo.section.tags.loading=正在載入標記…
metainfo.section.url.loading=正在載入 URL…
metainfo.section.url.label=物件 URL:
metainfo.section.presigned.url=預簽名 URL:
metainfo.headers.key=鍵
metainfo.headers.value=值
metainfo.headers.empty=無自訂標頭
metainfo.bucket.type=存儲桶類型:
metainfo.storage.class=存儲類別:
metainfo.status.loading=正在載入…
metainfo.versioning.status=版本控制:
metainfo.cross.replication=交叉複製
metainfo.cross.replication.empty=無數據
metainfo.tags.key=鍵
metainfo.tags.value=值
metainfo.tags.empty=無自訂標記
metainfo.bucket.region=區域:
metainfo.section.presigned.url.estimate=生成
alibaba.bucket.create.dialog.versioning=啟用版本控制
file.info.label.prefix=前綴:
settings.s3.region.group=AWS S3
settings.s3.region.group.label=區域:
settings.s3.region.group.china=AWS 中國
settings.s3.region.group.global=AWS 全球
settings.s3.region.group.us.gov=AWS GovCloud