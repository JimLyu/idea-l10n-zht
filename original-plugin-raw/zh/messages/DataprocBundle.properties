
cluster.action.stop=终止集群…
cluster.details.not.found=找不到集群 {0} 的信息。请刷新 EMR 集群列表。
cluster.state.terminated.last.week=已于上周终止
cluster.state.terminated.last.day=已于前一天终止
cluster.state.terminated.last.hour=已于前一小时终止
cluster.state.terminated=已终止
cluster.state.running=有效
cluster.state.starting=正在启动
metainfo.cluster.name=名称:
metainfo.cluster.id=ID:
metainfo.cluster.status=状态:
cluster.state.failed=已失败
group.name.dataproc=GC Dataproc
emr.remove.linked.connections.title=Dataproc 连接
dataproc.toolwindow.title=GC Dataproc
cluster.info.summary.name=名称:
cluster.info.summary.uiid=集群 UUID:
cluster.info.summary.type=类型:
cluster.info.summary.state=状态:
cluster.info.summary.state.details=状态详细信息:
cluster.info.config.region=区域:
cluster.info.config.zone=可用区
cluster.info.config.autoscaling=自动扩缩:
cluster.info.config.metastore=Dataproc 元存储:
cluster.info.config.scheduled.deletion=定时删除:
info.value.off=关闭
cluster.info.config.master.node.desc=主节点:
instance.config.machineType=机器类型:
instance.config.gpu.number=GPU 数量
instance.config.primary.disk.type=主磁盘类型:
instance.config.primary.disk.size=主磁盘大小:
instance.config.local.ssd=本地 SSD
cluster.info.config.worker.node.desc=工作进程节点:
cluster.info.config.secure.boot=安全启动:
cluster.info.config.vtpm=VTPM:
cluster.info.config.monitoring=完整性监控:
cluster.info.config.bucket=Cloud Storage 暂存存储分区:
cluster.info.config.network=网络:
cluster.info.internal.ip=仅限内部 IP:
cluster.info.image.version=映像版本:
cluster.info.image.created=创建时间:
cluster.info.optional.components=可选组件:
cluster.info.properties=属性:
cluster.info.labels=标签:
cluster.tab.vb.instances.title=VM 实例
cluster.tab.jobs.title=作业
cluster.tab.info.title=信息
cluster.tab.applications.title=应用程序
job.info.jobId=作业 ID:
job.info.jobUuid=作业 UUID:
job.info.status=状态:
job.info.status.details=状态详细信息:
job.info.start.date=开始日期:
job.info.elapsed.time=经过时间:
job.info.cluster=集群:
job.info.type=作业类型:
job.info.spark.args=实参:
job.info.labels=标签
job.info.spark.main.class.or.jar=主类或 jar
job.info.spark.jars=Jar:
job.info.spark.archives=归档:
job.info.spark.files=文件:
job.info.spark.main.pyfile=主 Python 文件:
job.info.spark.main.r.file=主 R 文件:
job.info.query.file=查询:
job.info.query.type=查询源:
job.info.query.file.value=查询文件:
job.info.query.text.value=查询文本:
job.info.properties=属性
action.open.cluster.log=打开 GC 日志目录
job.info.continue.on.failure=失败时继续
job.info.client.tags=客户端标记
job.hadoop.title=Hadoop
job.spark.title=Spark
job.spark.r.title=SparkR
job.pyspark.title=PySpark
job.hive.title=Hive
job.spark.sql.title=SparkSql
job.pig.title=Pig
job.presto.title=Presto
add.job.title=提交作业
job.info.max.restart.per.hour=每小时最大重启次数
job.properties.block.title=属性
job.label.block.title=标签
action.add.job.title=提交作业
job.info.spark.main.class.or.jar.title=主类/文件路径
job.info.spark.jars.title=JAR
job.info.spark.archives.title=选择归档
default.gcs.connection.name=GC Dataproc 项目
job.validation.file.fs={0} 必须为带有 gs://、hdfs:// 或 file:// 前缀的文件
job.validation.file.archive={0} 必须为归档类型 .jar、.tar、.tar.gz、.tgz、.zip。
job.info.spark.archives.hint=归档文件已在 Spark 工作目录中提取。可以是带有 gs:// 前缀的 GCS 文件、集群上带有 hdfs:// 前缀的 HDFS 文件或集群上带有 file:// 前缀的本地文件。支持的文件类型包括: .jar、.tar、.tar.gz、.tgz、.zip。
job.info.spark.jars.hint=Jar 文件包含在 CLASSPATH 中。可以是带有 gs:// 前缀的 GCS 文件、集群上带有 hdfs:// 前缀的 HDFS 文件或集群上带有 file:// 前缀的本地文件。
job.info.spark.main.class.or.jar.title.hint=提供的或标准 jar 文件中类的完全限定名称(例如 com.example.wordcount)，或提供的 jar 文件以使用该 jar 文件的主类
job.info.max.restart.per.hour.hint=如果您不想在作业失败时自动重启，请留空。
job.info.spark.main.r.file.title=选择主 R 文件
job.info.spark.additional.r.files.title=选择附加 R 文件
job.info.spark.additional.r.files=附加 R 文件:
job.info.single.file.hint=可以是带有 gs:// 前缀的 GCS 文件、集群上带有 hdfs:// 前缀的 HDFS 文件或集群上带有 file:// 前缀的本地文件
job.info.spark.additional.py.files.title=选择附加 Py 文件
job.info.spark.additional.py.files=附加 Python 文件:
job.info.spark.main.py.file.title=选择主 Py 文件
job.query.source.file=文件
job.query.source.text=文本
job.query.source.type=查询类型:
job.query.file.label=查询文件:
job.query.file.dialog.title=选择查询文件:
job.query.text.label=查询文本:
job.query.text.hint=要执行的查询
auth.process.wait.authorization=GCloud 授权